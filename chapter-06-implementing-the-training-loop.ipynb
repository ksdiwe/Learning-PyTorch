{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0643242d",
   "metadata": {},
   "source": [
    "# Implemting the Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1b6310",
   "metadata": {},
   "source": [
    "### Setting Up the Model, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f3d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Determine the available device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Move the model to the chosen device\n",
    "# model.to(device)\n",
    "\n",
    "# Executing model.to(device) modifies the model in place, \n",
    "# moving all its parameters and buffers to the GPU memory if CUDA is available, \n",
    "# otherwise keeping them on the CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af1c2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "\n",
    "# For multi-class classification\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# For regression problems (predicting continuous values)\n",
    "# loss_fn = torch.nn.MSELoss() # Mean Squared Error Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb63c0",
   "metadata": {},
   "source": [
    "When initializing an optimizer, you must provide two essential arguments:\n",
    "\n",
    "* **The model's parameters**: You tell the optimizer which tensors it should update. This is easily done using `model.parameters()`, which returns an iterator over all trainable parameters in the model.\n",
    "* **The learning rate (`lr`)**: This hyperparameter controls the step size for parameter updates. Finding a good learning rate is important for effective training. It often requires experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90dbaf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Using Stochastic Gradient Descent (SGD)\n",
    "learning_rate = 0.01\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Alternatively, using the Adam optimizer\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17afb10a",
   "metadata": {},
   "source": [
    "### Iterating Through Data with DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1cdbfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume these are already defined and configured:\n",
    "# train_dataloader = DataLoader(your_dataset, batch_size=64, shuffle=True)\n",
    "# model = YourNeuralNetwork()\n",
    "# loss_fn = torch.nn.CrossEntropyLoss() # Example loss function\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # Example optimizer\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device) # Ensure model is on the correct device\n",
    "\n",
    "num_epochs = 10 # Example number of passes over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f78374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Outer loop for epochs\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "\n",
    "#     # Set the model to training mode.\n",
    "#     # This enables features like dropout and batch normalization updates.\n",
    "#     model.train()\n",
    "\n",
    "#     # Inner loop for batches within an epoch\n",
    "#     # Iterate over batches provided by the DataLoader\n",
    "#     for batch_idx, data_batch in enumerate(train_dataloader):\n",
    "#         # 1. Unpack the batch\n",
    "#         # The structure depends on your Dataset's __getitem__ method.\n",
    "#         # For supervised learning, it's commonly (inputs, labels).\n",
    "#         inputs, labels = data_batch\n",
    "\n",
    "#         # 2. Move data to the target device (GPU or CPU)\n",
    "#         # This MUST match the device where the model resides.\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         # ---> The next steps (forward pass, loss, backprop, optimize) <---\n",
    "#         # ---> using 'inputs' and 'labels' happen here.             <---\n",
    "#         # (These are detailed in the subsequent sections)\n",
    "\n",
    "#         # Example placeholder for where subsequent logic goes:\n",
    "#         # predictions = model(inputs)\n",
    "#         # loss = loss_fn(predictions, labels)\n",
    "#         # optimizer.zero_grad()\n",
    "#         # loss.backward()\n",
    "#         # optimizer.step()\n",
    "\n",
    "#         # Optional: Print progress periodically\n",
    "#         if batch_idx % 100 == 0:\n",
    "#             current_batch_size = len(inputs) # Get size of the current batch\n",
    "#             # Replace 0.0 with the actual calculated loss for logging\n",
    "#             current_loss = 0.0\n",
    "#             print(f\"  Batch {batch_idx}: [{current_batch_size} samples] Current Loss: {current_loss:.4f}\") # Example log\n",
    "\n",
    "#     # ---> Evaluation loop on validation data often follows here <---\n",
    "#     # (We'll cover evaluation loops later in this chapter)\n",
    "\n",
    "# print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4bc61f",
   "metadata": {},
   "source": [
    "### The Forward Pass: Getting Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff2f0ac",
   "metadata": {},
   "source": [
    "### Calculating the Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af90832",
   "metadata": {},
   "source": [
    "### Backpropagation: Computing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318936c4",
   "metadata": {},
   "source": [
    "### Updating Weights with the Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d9729",
   "metadata": {},
   "source": [
    "### Zeroing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74726705",
   "metadata": {},
   "source": [
    "### Implementing an Evaluation Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1c676",
   "metadata": {},
   "source": [
    "### Saving and Loading Model Checkpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
