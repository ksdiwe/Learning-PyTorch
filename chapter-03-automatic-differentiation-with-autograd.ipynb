{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610f9faa",
   "metadata": {},
   "source": [
    "# Automatic Differentiation with Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c140e",
   "metadata": {},
   "source": [
    "PyTorch's Autograd system uses reverse-mode automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114a8d2",
   "metadata": {},
   "source": [
    "## PyTorch Computation Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9590de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result z: 7.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Tensors that require gradients\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "w = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# Operations\n",
    "y = w * x  # Intermediate result 'y'\n",
    "z = y + b  # Final result 'z'\n",
    "\n",
    "print(f\"Result z: {z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb01c27",
   "metadata": {},
   "source": [
    "## Tensors and Gradient Calculation (requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f73782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor x: tensor([1., 2., 3.])\n",
      "x.requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "# By default, when you create a tensor, its requires_grad attribute is set to False.\n",
    "# Default behavior: requires_grad is False\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f\"Tensor x: {x}\")\n",
    "print(f\"x.requires_grad: {x.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12577d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor y: tensor([4., 5., 6.])\n",
      "y.requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "# Create another tensor explicitly setting requires_grad to False\n",
    "y = torch.tensor([4.0, 5.0, 6.0], requires_grad=False)\n",
    "print(f\"\\nTensor y: {y}\")\n",
    "print(f\"y.requires_grad: {y.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa58d6",
   "metadata": {},
   "source": [
    "To instruct PyTorch to track operations and prepare for gradient computation for a specific tensor, you set its `requires_grad` attribute to `True`. There are two primary ways to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d9b38a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor w: tensor([ 0.5000, -1.0000], requires_grad=True)\n",
      "w.requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# 1. During Tensor Creation: Pass requires_grad=True as an argument \n",
    "# to the tensor creation function.\n",
    "\n",
    "# Enable gradient tracking at creation time\n",
    "w = torch.tensor([0.5, -1.0], requires_grad=True)\n",
    "print(f\"Tensor w: {w}\")\n",
    "print(f\"w.requires_grad: {w.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee0d0eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor b (before): tensor([0.1000])\n",
      "b.requires_grad (before): False\n",
      "\n",
      "Tensor b (after): tensor([0.1000], requires_grad=True)\n",
      "b.requires_grad (after): True\n"
     ]
    }
   ],
   "source": [
    "# 2. After Tensor Creation (In-place): Use the in-place method .requires_grad_(True) \n",
    "# on an existing tensor.\n",
    "\n",
    "b = torch.tensor([0.1])\n",
    "print(f\"Tensor b (before): {b}\")\n",
    "print(f\"b.requires_grad (before): {b.requires_grad}\")\n",
    "\n",
    "# Enable gradient tracking after creation\n",
    "b.requires_grad_(True)\n",
    "print(f\"\\nTensor b (after): {b}\")\n",
    "print(f\"b.requires_grad (after): {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acce531c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error setting requires_grad on integer tensor: Only Tensors of floating point and complex dtype can require gradients\n"
     ]
    }
   ],
   "source": [
    "# Attempting to set requires_grad=True on integer tensors will usually result in an error or may behave unexpectedly, \n",
    "# as gradients are not defined for discrete values in the same way.\n",
    "\n",
    "# Attempting requires_grad on an integer tensor\n",
    "try:\n",
    "    int_tensor = torch.tensor([1, 2], dtype=torch.int64, requires_grad=True)\n",
    "    # This line might not error immediately, but subsequent backward() calls involving it would.\n",
    "    print(f\"Integer tensor created with requires_grad=True: {int_tensor.requires_grad}\")\n",
    "    # Let's try a simple operation that might lead to issues later\n",
    "    result = int_tensor * 2.0 # Multiply by float to see if it causes issues\n",
    "    print(f\"Result requires_grad: {result.requires_grad}\")\n",
    "    # result.backward() # This would likely fail if we tried to backpropagate\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\nError setting requires_grad on integer tensor: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "414d80f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Float tensor created with requires_grad=True: True\n"
     ]
    }
   ],
   "source": [
    "# Best practice: Use float tensors for parameters/computations needing gradients\n",
    "float_tensor = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "print(f\"\\nFloat tensor created with requires_grad=True: {float_tensor.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f7a471c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x requires_grad: False\n",
      "w requires_grad: True\n",
      "b requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# Define tensors: x (input), w (weight), b (bias)\n",
    "x = torch.tensor([1.0, 2.0]) # Input data, gradients not needed\n",
    "w = torch.tensor([0.5, -1.0], requires_grad=True) # Weight parameter, track gradients\n",
    "b = torch.tensor([0.1], requires_grad=True) # Bias parameter, track gradients\n",
    "\n",
    "print(f\"x requires_grad: {x.requires_grad}\")\n",
    "print(f\"w requires_grad: {w.requires_grad}\")\n",
    "print(f\"b requires_grad: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80a57ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "intermediate (w * x) requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# Perform an operation: y = w * x + b\n",
    "# Note: PyTorch handles broadcasting for b\n",
    "intermediate = w * x\n",
    "print(f\"\\nintermediate (w * x) requires_grad: {intermediate.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfbd3605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "y = intermediate + b\n",
    "print(f\"y requires_grad: {y.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa4e4b9",
   "metadata": {},
   "source": [
    "Tensors created directly by the user (like our `x, w, and b` examples above) are considered \"leaf\" tensors in the graph. If they have `requires_grad=True`, their `.grad_fn` is `None` because they weren't created by a tracked operation within the graph. Tensors resulting from operations on tensors requiring gradients are \"non-leaf\" tensors and will have a `.grad_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c09cd1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x.grad_fn: None\n",
      "w.grad_fn: None\n",
      "b.grad_fn: None\n",
      "intermediate.grad_fn: <MulBackward0 object at 0x7e2c917035b0>\n",
      "y.grad_fn: <AddBackward0 object at 0x7e2c917035b0>\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nx.grad_fn: {x.grad_fn}\")\n",
    "print(f\"w.grad_fn: {w.grad_fn}\")\n",
    "print(f\"b.grad_fn: {b.grad_fn}\")\n",
    "print(f\"intermediate.grad_fn: {intermediate.grad_fn}\") # Result of multiplication\n",
    "print(f\"y.grad_fn: {y.grad_fn}\") # Result of addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b11068d",
   "metadata": {},
   "source": [
    "## Performing Backpropagation (backward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f987f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for x before backward: None\n",
      "Gradient for w before backward: None\n",
      "Gradient for b before backward: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example setup (imagine these are results from a model)\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "w = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# Perform some operations (building the graph)\n",
    "y = w * x + b  # y = 3*2 + 1 = 7\n",
    "loss = y * y   # loss = 7*7 = 49 (a scalar)\n",
    "\n",
    "# Before backward pass, gradients are None\n",
    "print(f\"Gradient for x before backward: {x.grad}\")\n",
    "print(f\"Gradient for w before backward: {w.grad}\")\n",
    "print(f\"Gradient for b before backward: {b.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6003d698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for x after backward: 42.0\n",
      "Gradient for w after backward: 28.0\n",
      "Gradient for b after backward: 14.0\n"
     ]
    }
   ],
   "source": [
    "# Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# After backward pass, gradients are populated\n",
    "print(f\"Gradient for x after backward: {x.grad}\") # d(loss)/dx = d(y^2)/dx = 2*y*(dy/dx) = 2*y*w = 2*7*3 = 42\n",
    "print(f\"Gradient for w after backward: {w.grad}\") # d(loss)/dw = d(y^2)/dw = 2*y*(dy/dw) = 2*y*x = 2*7*2 = 28\n",
    "print(f\"Gradient for b after backward: {b.grad}\") # d(loss)/db = d(y^2)/db = 2*y*(dy/db) = 2*y*1 = 2*7*1 = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce7002c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing the previous example, but with a non-scalar y\n",
    "x_vector = torch.tensor([2.0, 4.0], requires_grad=True)\n",
    "w = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(1.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d742d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_non_scalar = w * x_vector + b # y_non_scalar is now a non-scalar tensor with two elements: [7.0, 13.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e3e84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error calling backward() on non-scalar: grad can be implicitly created only for scalar outputs\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    y_non_scalar.backward() # This will cause an error\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error calling backward() on non-scalar: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c9bd056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for x_vector after y_non_scalar.backward(gradient=...): tensor([3., 3.])\n",
      "Gradient for w after y_non_scalar.backward(gradient=...): 6.0\n"
     ]
    }
   ],
   "source": [
    "# To make it work, provide a gradient tensor matching y_non_scalar's shape\n",
    "# This represents the gradient of some final loss w.r.t y.\n",
    "# For demonstration, let's use torch.ones_like(y_non_scalar)\n",
    "grad_tensor = torch.ones_like(y_non_scalar)\n",
    "y_non_scalar.backward(gradient=grad_tensor)\n",
    "print(f\"Gradient for x_vector after y_non_scalar.backward(gradient=...): {x_vector.grad}\")\n",
    "print(f\"Gradient for w after y_non_scalar.backward(gradient=...): {w.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a886788c",
   "metadata": {},
   "source": [
    "By default, PyTorch frees the intermediate buffers of the computation graph after `backward()` is called to save memory. \n",
    "\n",
    "This means if you need to call `backward()` multiple times on the same part of the graph (less common, often needed for advanced techniques or debugging), you would need to pass `retain_graph=True` to the first `backward()` call. However, for standard training, you construct a graph, compute loss, call `backward()`, update weights, and then repeat the process for the next batch, which builds a new graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8b28b",
   "metadata": {},
   "source": [
    "## Accessing Gradients (.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f48b1dd",
   "metadata": {},
   "source": [
    "leaf tensors are typically the ones you created directly, like model parameters or inputs, as opposed to intermediate tensors resulting from operations."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "09530f65",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924a6830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create input tensors that require gradients\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "w = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# Define a simple computation\n",
    "y = w * x + b  # y = 3.0 * 2.0 + 1.0 = 7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1c82cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of y with respect to x (dy/dx): 3.0\n",
      "Gradient of y with respect to w (dy/dw): 2.0\n",
      "Gradient of y with respect to b (dy/db): 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compute gradients\n",
    "y.backward()\n",
    "\n",
    "# Access the gradients stored in the .grad attribute\n",
    "print(f\"Gradient of y with respect to x (dy/dx): {x.grad}\")\n",
    "print(f\"Gradient of y with respect to w (dy/dw): {w.grad}\")\n",
    "print(f\"Gradient of y with respect to b (dy/db): {b.grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "917e7d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for tensor z (requires_grad=False): None\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor that does NOT require gradients\n",
    "z = torch.tensor(4.0, requires_grad=False)\n",
    "print(f\"Gradient for tensor z (requires_grad=False): {z.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b4a95",
   "metadata": {},
   "source": [
    "## Disabling Gradient Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6113f254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.requires_grad: True\n",
      "y.grad_fn: <AddBackward0 object at 0x7dd88bd3d9f0>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensors\n",
    "x = torch.randn(2, 2, requires_grad=True)\n",
    "w = torch.randn(2, 2, requires_grad=True)\n",
    "b = torch.randn(2, 2, requires_grad=True)\n",
    "\n",
    "# Operation outside the no_grad context\n",
    "y = x * w + b\n",
    "print(f\"y.requires_grad: {y.requires_grad}\") \n",
    "print(f\"y.grad_fn: {y.grad_fn}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfcff102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entering torch.no_grad() context:\n",
      "  z.requires_grad: False\n",
      "  z.grad_fn: None\n",
      "  k.requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "# Perform operations within the no_grad context\n",
    "print(\"\\nEntering torch.no_grad() context:\")\n",
    "with torch.no_grad():\n",
    "    z = x * w + b\n",
    "    print(f\"  z.requires_grad: {z.requires_grad}\") # Output: z.requires_grad: False\n",
    "    print(f\"  z.grad_fn: {z.grad_fn}\")           # Output: z.grad_fn: None\n",
    "\n",
    "    # Even if an input requires grad, the output won't\n",
    "    k = x * 5\n",
    "    print(f\"  k.requires_grad: {k.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "007168bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exiting torch.no_grad() context:\n",
      "p.requires_grad: True\n",
      "p.grad_fn: <MulBackward0 object at 0x7dd979f19a80>\n"
     ]
    }
   ],
   "source": [
    "# Outside the context, tracking resumes if inputs require grad\n",
    "print(\"\\nExiting torch.no_grad() context:\")\n",
    "p = x * w\n",
    "print(f\"p.requires_grad: {p.requires_grad}\") \n",
    "print(f\"p.grad_fn: {p.grad_fn}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6e4a79",
   "metadata": {},
   "source": [
    "no associated grad_fn, indicating they are detached from the computation graph history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ea3868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluation loop snippet\n",
    "# model.eval() # Set model to evaluation mode (important for layers like dropout, batchnorm)\n",
    "# total_loss = 0\n",
    "# correct_predictions = 0\n",
    "\n",
    "# with torch.no_grad(): # Disable gradient calculations for evaluation\n",
    "#     for inputs, labels in validation_dataloader:\n",
    "#         inputs, labels = inputs.to(device), labels.to(device) # Move data to appropriate device\n",
    "\n",
    "#         outputs = model(inputs) # Forward pass\n",
    "#         loss = criterion(outputs, labels) # Calculate loss\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         correct_predictions += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d2cd881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.requires_grad: True\n",
      "b.grad_fn: <MulBackward0 object at 0x7dd97bf735b0>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Original tensor requiring gradients\n",
    "a = torch.randn(3, 3, requires_grad=True)\n",
    "b = a * 2\n",
    "print(f\"b.requires_grad: {b.requires_grad}\") \n",
    "print(f\"b.grad_fn: {b.grad_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f94e125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After detaching b to create c:\n",
      "c.requires_grad: False\n",
      "c.grad_fn: None\n"
     ]
    }
   ],
   "source": [
    "# Detach the tensor 'b'\n",
    "c = b.detach()\n",
    "print(f\"\\nAfter detaching b to create c:\")\n",
    "print(f\"c.requires_grad: {c.requires_grad}\") \n",
    "print(f\"c.grad_fn: {c.grad_fn}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d74a2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original tensor b remains attached:\n",
      "b.requires_grad: True\n",
      "b.grad_fn: <MulBackward0 object at 0x7dd88c2c4d60>\n"
     ]
    }
   ],
   "source": [
    "# Importantly, the original tensor 'b' is unchanged\n",
    "print(f\"\\nOriginal tensor b remains attached:\")\n",
    "print(f\"b.requires_grad: {b.requires_grad}\") \n",
    "print(f\"b.grad_fn: {b.grad_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b256c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Operation on detached tensor c:\n",
      "d.requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "# Operations using the detached tensor 'c' won't be tracked\n",
    "d = c + 1\n",
    "print(f\"\\nOperation on detached tensor c:\")\n",
    "print(f\"d.requires_grad: {d.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a93ff62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial requires_grad: True\n",
      "After requires_grad_(False): False\n"
     ]
    }
   ],
   "source": [
    "# modifying requires_grad In-place\n",
    "import torch\n",
    "my_tensor = torch.randn(5, requires_grad=True)\n",
    "print(f\"Initial requires_grad: {my_tensor.requires_grad}\")\n",
    "\n",
    "# Disable gradient tracking in-place\n",
    "my_tensor.requires_grad_(False) # Note the underscore for in-place operation\n",
    "print(f\"After requires_grad_(False): {my_tensor.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c5d0a",
   "metadata": {},
   "source": [
    "## Gradient Accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6967b10",
   "metadata": {},
   "source": [
    "When calling `.backward()` multiple times without clearing the gradients in between, PyTorch adds the newly computed gradients to the existing values stored in the `.grad` attribute of the leaf tensors (parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5dadb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After first backward pass, x.grad: tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor that requires gradients\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Perform some operations\n",
    "y = x * x\n",
    "z = y * 3 # z = 3 * x^2\n",
    "\n",
    "# First backward pass\n",
    "# dz/dx = 6*x = 6*2 = 12\n",
    "z.backward(retain_graph=True) # retain_graph=True allows subsequent backward calls\n",
    "print(f\"After first backward pass, x.grad: {x.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7df0319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After second backward pass, x.grad: tensor([24.])\n"
     ]
    }
   ],
   "source": [
    "# Perform another operation (can be the same or different)\n",
    "# For simplicity, let's use the same z again for demonstration\n",
    "# Note: In a real scenario, you'd likely compute a new loss\n",
    "# based on a new input or different part of the model.\n",
    "z.backward() # Second backward pass\n",
    "# We expect the new gradient (12) to be added to the existing one (12)\n",
    "print(f\"After second backward pass, x.grad: {x.grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95963dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After zeroing, x.grad: tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "# Manually zero the gradient\n",
    "x.grad.zero_()\n",
    "print(f\"After zeroing, x.grad: {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25f8a0b",
   "metadata": {},
   "source": [
    "## Autograd Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47a2dabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([2., 4., 6.])\n",
      "w: tensor([0.5000], requires_grad=True)\n",
      "x.requires_grad: False\n",
      "w.requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "x = torch.tensor([2.0, 4.0, 6.0])\n",
    "\n",
    "# Weight tensor - requires gradient computation\n",
    "w = torch.tensor([0.5], requires_grad=True)\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"w: {w}\")\n",
    "print(f\"x.requires_grad: {x.requires_grad}\")\n",
    "print(f\"w.requires_grad: {w.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3078d8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([1., 2., 3.], grad_fn=<MulBackward0>)\n",
      "L: 2.0\n",
      "y.requires_grad: True\n",
      "L.requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# Forward pass: y = w * x\n",
    "y = w * x\n",
    "\n",
    "# Define a simple scalar loss L (e.g., mean of y)\n",
    "L = y.mean()\n",
    "\n",
    "print(f\"y: {y}\")\n",
    "print(f\"L: {L}\")\n",
    "print(f\"y.requires_grad: {y.requires_grad}\")\n",
    "print(f\"L.requires_grad: {L.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3774b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform backpropagation\n",
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5b79eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient dL/dw: tensor([4.])\n",
      "Gradient dL/dx: None\n"
     ]
    }
   ],
   "source": [
    "# Gradient is stored in w.grad\n",
    "print(f\"Gradient dL/dw: {w.grad}\")\n",
    "\n",
    "# x did not require gradients, so its gradient is None\n",
    "print(f\"Gradient dL/dx: {x.grad}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
