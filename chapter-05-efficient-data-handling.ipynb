{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e663231a",
   "metadata": {},
   "source": [
    "# Efficient Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd32a7",
   "metadata": {},
   "source": [
    "### Working with `torch.utils.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa05ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class SimpleCustomDataset(Dataset):\n",
    "    \"\"\"A simple example dataset with features and labels.\"\"\"\n",
    "\n",
    "    def __init__(self, features, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (list or np.array): A list or array of features.\n",
    "            labels (list or np.array): A list or array of labels.\n",
    "        \"\"\"\n",
    "        # Basic check: Features and labels must have the same length\n",
    "        assert len(features) == len(labels), \"Features and labels must have the same length.\"\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generates one sample of data.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the item.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (feature, label) for the given index.\n",
    "        \"\"\"\n",
    "        # Retrieve feature and label for the given index\n",
    "        feature = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Often, you'll convert data to PyTorch tensors here\n",
    "        # We assume features/labels might not be tensors yet\n",
    "        sample = (torch.tensor(feature, dtype=torch.float32),\n",
    "                  torch.tensor(label, dtype=torch.long)) # Assuming classification label\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39cfa118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 100\n",
      "\n",
      "First sample features:\n",
      "tensor([-0.6250,  0.5624, -0.0776,  1.0968,  0.5423, -0.1851,  1.3632, -0.9676,\n",
      "         0.1250, -1.5552])\n",
      "First sample shape: torch.Size([10])\n",
      "First sample label: 0\n",
      "\n",
      "Tenth sample label: 4\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "# Sample data (replace with your actual data)\n",
    "num_samples = 100\n",
    "num_features = 10\n",
    "features_data = np.random.randn(num_samples, num_features)\n",
    "labels_data = np.random.randint(0, 5, size=num_samples) # Example: 5 classes\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "my_dataset = SimpleCustomDataset(features_data, labels_data)\n",
    "\n",
    "# Access dataset properties and elements\n",
    "print(f\"Dataset size: {len(my_dataset)}\")\n",
    "\n",
    "# Get the first sample\n",
    "first_sample = my_dataset[0]\n",
    "feature_sample, label_sample = first_sample\n",
    "print(f\"\\nFirst sample features:\\n{feature_sample}\")\n",
    "print(f\"First sample shape: {feature_sample.shape}\")\n",
    "print(f\"First sample label: {label_sample}\")\n",
    "\n",
    "# Get the tenth sample\n",
    "tenth_sample = my_dataset[9]\n",
    "print(f\"\\nTenth sample label: {tenth_sample[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a13b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image # Python Imaging Library for image loading\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class ImageFilelistDataset(Dataset):\n",
    "    \"\"\"Dataset for loading image paths and labels from a CSV file.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "                               Assumes columns: 'image_path', 'label'\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                                           on a sample.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform # We'll discuss transforms later\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image path relative to root_dir from the CSV\n",
    "        img_rel_path = self.annotations.iloc[idx, 0] # Assuming first column is path\n",
    "        img_full_path = os.path.join(self.root_dir, img_rel_path)\n",
    "\n",
    "        # Load the image using PIL\n",
    "        try:\n",
    "            image = Image.open(img_full_path).convert('RGB') # Ensure 3 channels\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Image not found at {img_full_path}\")\n",
    "            # Handle error appropriately, e.g., return None or raise exception\n",
    "            # For simplicity here, we'll return None and rely on DataLoader's collate_fn\n",
    "            # to potentially handle it (or filter later). A better approach\n",
    "            # might be to clean the CSV beforehand.\n",
    "            return None, None # Returning None values\n",
    "\n",
    "        # Get the label from the CSV\n",
    "        label = self.annotations.iloc[idx, 1] # Assuming second column is label\n",
    "        label = torch.tensor(int(label), dtype=torch.long)\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image) # Transforms usually convert PIL Image to Tensor\n",
    "\n",
    "        # If no transform is provided that converts to tensor, do it manually\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "             # Basic conversion if no other transform applied\n",
    "             image = torch.tensor(np.array(image), dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23cb519",
   "metadata": {},
   "source": [
    "### Built-in Datasets (e.g., TorchVision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48d60469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:01<00:00, 106MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 training dataset size: 50000\n",
      "CIFAR-10 test dataset size: 10000\n",
      "Image shape: torch.Size([3, 32, 32])\n",
      "Label: 6\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define a simple transformation to convert images to PyTorch Tensors\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load the training dataset\n",
    "# root: directory where data will be stored/found\n",
    "# train=True: specifies the training set\n",
    "# download=True: downloads the data if not found locally\n",
    "# transform: applies the defined transformation to each image\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                             train=True,\n",
    "                                             download=True,\n",
    "                                             transform=transform)\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "print(f\"CIFAR-10 training dataset size: {len(train_dataset)}\")\n",
    "print(f\"CIFAR-10 test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Accessing a single data point (image, label)\n",
    "img, label = train_dataset[0]\n",
    "print(f\"Image shape: {img.shape}\") \n",
    "print(f\"Label: {label}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641a774",
   "metadata": {},
   "source": [
    "### Data Transformations (`torchvision.transforms`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617a5ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Example transform pipeline for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),             # Resize smaller edge to 256\n",
    "    transforms.RandomCrop(224),         # Randomly crop a 224x224 patch\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip horizontally\n",
    "    transforms.ToTensor(),              # Convert PIL Image to tensor (0-1 range)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # Normalize with ImageNet stats\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d96b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example transform pipeline for validation/testing (no augmentation)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),         # Center crop to 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be1a1778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Transforms:\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    RandomCrop(size=(224, 224), padding=None)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "\n",
      "Testing Transforms:\n",
      "Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Transforms:\")\n",
    "print(train_transform)\n",
    "\n",
    "print(\"\\nTesting Transforms:\")\n",
    "print(test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686fe4c7",
   "metadata": {},
   "source": [
    "### Using `torch.utils.data.DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9995951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For demonstration, let's create a simple dummy dataset:\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, num_samples=100):\n",
    "        self.num_samples = num_samples\n",
    "        self.features = torch.randn(num_samples, 10) # Example: 100 samples, 10 features\n",
    "        self.labels = torch.randint(0, 2, (num_samples,)) # Example: 100 binary labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46347a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the dataset\n",
    "dataset = DummyDataset(num_samples=105)\n",
    "\n",
    "# Instantiate the DataLoader\n",
    "# batch_size: Number of samples per batch\n",
    "# shuffle: Set to True to shuffle data every epoch (important for training)\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df0c0719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 105\n",
      "DataLoader batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the DataLoader\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"DataLoader batch size: {train_loader.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44f3c8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1 ---\n",
      "Batch 1: Features shape=torch.Size([32, 10]), Labels shape=torch.Size([32])\n",
      "Batch 2: Features shape=torch.Size([32, 10]), Labels shape=torch.Size([32])\n",
      "Batch 3: Features shape=torch.Size([32, 10]), Labels shape=torch.Size([32])\n",
      "Batch 4: Features shape=torch.Size([9, 10]), Labels shape=torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1): # Example for one epoch\n",
    "    print(f\"\\n--- Epoch {epoch+1} ---\")\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # DataLoader yields batches. Each 'batch' is typically a tuple or list\n",
    "        # containing tensors for features and labels.\n",
    "        features, labels = batch\n",
    "        print(f\"Batch {i+1}: Features shape={features.shape}, Labels shape={labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e88ec6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DataLoader with drop_last=True ---\n",
      "Batch 1: Features shape=torch.Size([32, 10]), Labels shape=torch.Size([32])\n",
      "Batch 2: Features shape=torch.Size([32, 10]), Labels shape=torch.Size([32])\n",
      "Batch 3: Features shape=torch.Size([32, 10]), Labels shape=torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# If you prefer all batches to have the exact batch_size, \n",
    "# discarding the smaller last batch, \n",
    "# you can set drop_last=True when creating the DataLoader:\n",
    "\n",
    "# Drop the last incomplete batch if dataset size is not divisible by batch size\n",
    "train_loader_drop_last = DataLoader(dataset=dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "print(\"\\n--- DataLoader with drop_last=True ---\")\n",
    "for i, batch in enumerate(train_loader_drop_last):\n",
    "    features, labels = batch\n",
    "    print(f\"Batch {i+1}: Features shape={features.shape}, Labels shape={labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51da84ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use 4 worker processes for data loading\n",
    "# num_workers > 0 enables multi-process data loading\n",
    "# A common starting point is num_workers = 4 * num_gpus, but optimal value depends\n",
    "# on the system (CPU cores, disk speed) and batch size. Experimentation is often needed.\n",
    "fast_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "# Iteration looks the same, but data loading happens in background processes\n",
    "# for features, labels in fast_loader:\n",
    "#     # Training steps...\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a0a967",
   "metadata": {},
   "source": [
    "_The `DataLoader` wraps the `Dataset` and, if `num_workers` `> 0`, uses worker processes to fetch and collate samples into batches, which are then consumed by the training loop._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30913062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable pinned memory for faster CPU-to-GPU transfers\n",
    "gpu_optimized_loader = DataLoader(dataset=dataset,\n",
    "                                batch_size=32,\n",
    "                                shuffle=True,\n",
    "                                num_workers=4,\n",
    "                                pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62d9e97",
   "metadata": {},
   "source": [
    "Setting `pin_memory=True` instructs the `DataLoader` to allocate the tensors in \"pinned\" (page-locked) memory on the CPU side. \n",
    "Transfers from pinned CPU memory to GPU memory are generally faster than from standard pageable CPU memory. This is most effective when used in conjunction with `num_workers > 0`. \n",
    "\n",
    "**Note that using pinned memory consumes more CPU RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02233ce7",
   "metadata": {},
   "source": [
    "### Customizing DataLoader Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c5d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# Assume 'dataset' is your torch.utils.data.Dataset instance\n",
    "# Assume 'targets' is a list or tensor containing the class label for each sample\n",
    "# e.g., targets = [0, 0, 1, 0, ..., 1, 0]\n",
    "\n",
    "# Calculate weights for each sample\n",
    "class_counts = torch.bincount(torch.tensor(targets)) # Counts per class: e.g., [900, 100]\n",
    "num_samples = len(targets) # Total samples: 1000\n",
    "\n",
    "# Weight for each sample is 1 / (number of samples in its class)\n",
    "sample_weights = torch.tensor([1.0 / class_counts[t] for t in targets])\n",
    "\n",
    "# Create the sampler\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=num_samples, replacement=True)\n",
    "\n",
    "# Create the DataLoader using the custom sampler\n",
    "# Note: shuffle must be False when using a sampler\n",
    "dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "\n",
    "# Now, batches drawn from this dataloader will have a more balanced\n",
    "# representation of classes over time.\n",
    "# for batch_features, batch_labels in dataloader:\n",
    "#     # Training steps...\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4648109d",
   "metadata": {},
   "source": [
    "A custom `collate_fn` can pad the sequences within each batch to the maximum length in that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f807541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Example Dataset returning variable-length tensors\n",
    "class VariableSequenceDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # data is a list of tensors, e.g., [torch.randn(5), torch.randn(8), ...]\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For simplicity, assume each item also has a label (e.g., its length)\n",
    "        sequence = self.data[idx]\n",
    "        label = len(sequence)\n",
    "        return sequence, label\n",
    "\n",
    "# Custom collate function\n",
    "def pad_collate(batch):\n",
    "    # batch is a list of tuples: [(sequence1, label1), (sequence2, label2), ...]\n",
    "    # Sort batch elements by sequence length (optional, but often done for RNN efficiency)\n",
    "    # batch.sort(key=lambda x: len(x[0]), reverse=True) # Not strictly necessary for padding\n",
    "\n",
    "    # Separate sequences and labels\n",
    "    sequences = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "\n",
    "    # Pad sequences to the length of the longest sequence in the batch\n",
    "    # `batch_first=True` makes the output shape (batch_size, max_seq_len, features)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    # Stack labels (assuming they are simple scalars)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return padded_sequences, labels\n",
    "\n",
    "# Create dataset and dataloader\n",
    "sequences = [torch.randn(torch.randint(5, 15, (1,)).item()) for _ in range(100)]\n",
    "dataset = VariableSequenceDataset(sequences)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, collate_fn=pad_collate)\n",
    "\n",
    "# Iterate through the dataloader\n",
    "# for padded_batch, label_batch in dataloader:\n",
    "#     # padded_batch shape: (4, max_len_in_this_batch, 1) if sequences were 1D\n",
    "#     # label_batch shape: (4,)\n",
    "#     # Model processing...\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23280fd",
   "metadata": {},
   "source": [
    "This custom `collate_fn` uses `torch.nn.utils.rnn.pad_sequence`to handle the padding, ensuring all sequences in the batch have the same length, making them suitable for processing by models like RNNs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
